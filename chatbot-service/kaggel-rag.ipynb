{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install needed dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence_transformers docling chromadb rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fastapi uvicorn nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If i  will run in my local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma db config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    ")\n",
    "client = chromadb.PersistentClient(\n",
    "    path = \"./.chroma_db\",\n",
    "    settings = Settings(),\n",
    "    tenant = DEFAULT_TENANT,\n",
    "    database = DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# user_collection = client.get_or_create_collection(\n",
    "#     name = \"user_collection\",\n",
    "#     embedding_function = sentence_transformer_ef,\n",
    "#     metadata = {\n",
    "#         \"hnsw:space\":\"cosine\",\n",
    "#         \"hnsw:search_ef\":100\n",
    "#     }\n",
    "# )\n",
    "\n",
    "general_collection = client.get_or_create_collection(\n",
    "    name = \"general_collection\",\n",
    "    embedding_function = sentence_transformer_ef,\n",
    "    metadata = {\n",
    "        \"hnsw:space\":\"cosine\",\n",
    "        \"hnsw:search_ef\":100\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docling scrapper config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.datamodel.base_models import InputFormat, DocumentStream\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.chunking import HybridChunker\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = True\n",
    "pipeline_options.do_table_structure = True\n",
    "pipeline_options.table_structure_options.do_cell_matching = True\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options = {\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options = pipeline_options, \n",
    "            backend = PyPdfiumDocumentBackend\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "MAX_TOKENS = 1000\n",
    "OVERLAP_TOKENS = 200\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_ID)\n",
    "chunker = HybridChunker(\n",
    "    tokmizer = tokenizer,\n",
    "    max_tokens = MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse user pdf, do embedding and store data to vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import UploadFile, WebSocket, HTTPException\n",
    "import uuid\n",
    "from io import BytesIO\n",
    "from docling.datamodel.base_models import InputFormat, DocumentStream\n",
    "async def upload_pdf(file: UploadFile,  store_type: str, indexName: str, websocket: WebSocket):\n",
    "    if not file:\n",
    "        raise HTTPException(status_code=400, detail=\"No file uploaded\")\n",
    "\n",
    "    if file.content_type != \"application/pdf\":\n",
    "        raise HTTPException(status_code=400, detail=\"Uploaded file is not a PDF\")\n",
    "    \n",
    "    if len(await file.read()) == 0:\n",
    "        raise HTTPException(status_code=400, detail=\"Uploaded file is empty\")\n",
    "        \n",
    "    file.file.seek(0)\n",
    "    file_name = file.filename\n",
    "    pdf_bytes = await file.read()\n",
    "    buf = BytesIO(pdf_bytes)\n",
    "\n",
    "    print(f\"Processing PDF: Scrapping content...\")\n",
    "    await websocket.send_text(\"Processing PDF: Scraping content...\")\n",
    "    source = DocumentStream(name=file.filename, stream=buf)\n",
    "    result = doc_converter.convert(source)\n",
    "    doc = result.document\n",
    "\n",
    "    print(f\"Processing PDF: Chunk results...\")\n",
    "    await websocket.send_text(\"Processing PDF: Chunking results...\")\n",
    "    chunk_iter = chunker.chunk(doc)\n",
    "    chunks = []\n",
    "    prev_text = \"\"\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        overlap_text = prev_text[-OVERLAP_TOKENS:] if len(prev_text) > OVERLAP_TOKENS else prev_text\n",
    "        combined_text = overlap_text + chunk.text\n",
    "        chunks.append({\"chunk_id\": i, \"text\": combined_text})\n",
    "        prev_text = chunk.text\n",
    "\n",
    "    print(f\"Processing PDF: Embedding results...\")\n",
    "    await websocket.send_text(\"Processing PDF: Embedding results...\")\n",
    "    embeddings = []\n",
    "    metadata_list = []\n",
    "    for chunk in chunks:\n",
    "        embedding = sentence_transformer_ef([chunk[\"text\"]]) \n",
    "        embeddings.append(embedding[0])\n",
    "        metadata = {\"chunk_id\": chunk[\"chunk_id\"], \"source\": file_name}\n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    unique_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "\n",
    "    print(f\"Processing PDF: Storing data in Vector DB...\")\n",
    "    await websocket.send_text(\"Processing PDF: Storing data in Vector DB (user index)...\")\n",
    "    user_collection = client.get_or_create_collection(\n",
    "        name = indexName,\n",
    "        embedding_function = sentence_transformer_ef,\n",
    "        metadata = {\n",
    "            \"hnsw:space\":\"cosine\",\n",
    "            \"hnsw:search_ef\":100\n",
    "        }\n",
    "    )\n",
    "    user_collection.add(\n",
    "        ids=unique_ids,\n",
    "        documents=[chunk[\"text\"] for chunk in chunks],\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadata_list\n",
    "    )\n",
    "    if store_type == \"user_and_general\":\n",
    "        await websocket.send_text(\"Processing PDF: Storing data in Vector DB (general index)...\")\n",
    "        general_collection.add(\n",
    "            ids=unique_ids,\n",
    "            documents=[chunk[\"text\"] for chunk in chunks],\n",
    "            embeddings=embeddings,\n",
    "            metadatas=metadata_list\n",
    "        )\n",
    "    await websocket.send_text(f\"Uploaded and indexed {len(chunks)} chunks from user PDF {file_name} to index {store_type}.\")\n",
    "    return f\"Uploaded and indexed {len(chunks)} chunks from user PDF {file_name} to index {store_type}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and store general data to vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-06T21:25:44.418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded and indexed 12085 chunks from JSON file.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# file_path = \"/kaggle/input/anatomy-data-json/decoded_output.json\"\n",
    "file_path = \"./data-json/decoded_output.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "embeddings = []\n",
    "metadata_list = []\n",
    "for item in data:\n",
    "    embedding = sentence_transformer_ef([item[\"text\"]]) \n",
    "    embeddings.append(embedding[0])\n",
    "    metadata = {\"chunk_id\": item[\"chunk_id\"], \"source\": item[\"source\"]}\n",
    "    metadata_list.append(metadata)\n",
    "general_collection.add(\n",
    "    ids=[str(index) for index, _ in enumerate(data)],\n",
    "    documents=[item[\"text\"] for item in data],\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadata_list\n",
    ")\n",
    "print(f\"Uploaded and indexed {len(data)} chunks from JSON file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrival and G enerator config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T20:27:32.720000Z",
     "iopub.status.busy": "2025-01-06T20:27:32.719713Z",
     "iopub.status.idle": "2025-01-06T20:27:35.737763Z",
     "shell.execute_reply": "2025-01-06T20:27:35.736787Z",
     "shell.execute_reply.started": "2025-01-06T20:27:32.719978Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "models = {\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\": None,\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": None,\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": None\n",
    "}\n",
    "\n",
    "tokenizers = {\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\": None,\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": None,\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": None\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    if model_name not in models:\n",
    "        raise ValueError(f\"Model {model_name} is not supported.\")\n",
    "    if models[model_name] is None:\n",
    "        models[model_name] = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        tokenizers[model_name] = AutoTokenizer.from_pretrained(model_name)\n",
    "    return models[model_name], tokenizers[model_name]\n",
    "\n",
    "def get_generate_response(query: str, collection_name: str, model_name: str, search_type: str):\n",
    "    print(\"in generating method\")\n",
    "    query_timestamp = datetime.utcnow().isoformat()\n",
    "    total_start_time = time.time()\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "\n",
    "    collection = client.get_collection(collection_name)\n",
    "\n",
    "    query_embedding = sentence_transformer_ef([query])[0]\n",
    "    print(\"start reterival\")\n",
    "    results = collection.query(\n",
    "                query_embeddings=query_embedding,\n",
    "                n_results=6\n",
    "            )\n",
    "    retrieval_time = time.time() - retrieval_start_time\n",
    "\n",
    "    documents = results.get(\"documents\", [[]])[0]\n",
    "    metadatas = results.get(\"metadatas\", [[]])[0]\n",
    "    similarities = results.get(\"distances\", [])[0]\n",
    "\n",
    "    context = \"\\n\".join(documents)\n",
    "    resources = [meta.get(\"source\", \"Unknown source\") for meta in metadatas]\n",
    "    unique_resources = list(set(resources))\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant that answers questions based on the provided context.\\n\"},\n",
    "        {\"role\": \"system\", \"content\": f\"Context: {context}\\n\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(\"begin generating a response\")\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    input_tokens = model_inputs.input_ids.shape[-1]\n",
    "\n",
    "    generation_start_time = time.time()\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generation_time = time.time() - generation_start_time \n",
    "\n",
    "    output_tokens = sum(len(ids) for ids in generated_ids)\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(\"end of generated\")\n",
    "    retrieved_docs = [\n",
    "            {\n",
    "                \"content\": doc,\n",
    "                \"similarity\": round(similarity * 100, 2),  # Convert to percentage\n",
    "                \"source\": meta.get(\"source\", \"Unknown source\")\n",
    "            }\n",
    "            for doc, similarity, meta in zip(documents, similarities, metadatas)\n",
    "    ]\n",
    "    total_time = time.time() - total_start_time\n",
    "    StackTrace = {\n",
    "        \"userQuery\": query,\n",
    "        \"chatbotResponse\": response,\n",
    "        \"retrievedDocs\": retrieved_docs,\n",
    "        \"inputTokens\": input_tokens,\n",
    "        \"outputTokens\": output_tokens,\n",
    "        \"queryTimestamp\": query_timestamp,\n",
    "        \"model_name\": model_name,\n",
    "        \"sources\": unique_resources,\n",
    "        \"performanceMetrics\": {\n",
    "            \"totalTime\": round(total_time, 4),\n",
    "            \"retrievalTime\": round(retrieval_time, 4),\n",
    "            \"generationTime\": round(generation_time, 4)\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    return response, unique_resources, StackTrace\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T20:45:33.560784Z",
     "iopub.status.busy": "2025-01-06T20:45:33.560479Z",
     "iopub.status.idle": "2025-01-06T20:45:46.660091Z",
     "shell.execute_reply": "2025-01-06T20:45:46.659106Z",
     "shell.execute_reply.started": "2025-01-06T20:45:33.560758Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"what is anatomy and Physiology?\"\n",
    "collection_name = \"general_collection\"\n",
    "search_type = \"general_collection\"\n",
    "response, unique_resources, StackTrace = get_generate_response(query, collection_name, \"Qwen/Qwen2.5-1.5B-Instruct\", search_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [1:01:23<00:00, 1841.95s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 17.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "models = {\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\": None,\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": None,\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": None\n",
    "}\n",
    "\n",
    "for model_name in models.keys():\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Store them in the dictionary if needed\n",
    "    # models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T20:45:50.031200Z",
     "iopub.status.busy": "2025-01-06T20:45:50.030902Z",
     "iopub.status.idle": "2025-01-06T20:45:50.036109Z",
     "shell.execute_reply": "2025-01-06T20:45:50.035417Z",
     "shell.execute_reply.started": "2025-01-06T20:45:50.031177Z"
    }
   },
   "outputs": [],
   "source": [
    "print(response)\n",
    "print(\"\\n -----------------------------------------------------------------------------\")\n",
    "print(unique_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_translation(query):\n",
    "    print(\"\\n ------------------------------- begin translation ----------------------------------------------\")\n",
    "    # Chat messages to guide translation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, a multilingual assistant. If the user provides a query, you should automatically translate it to user wanted language.\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    # Prepare the text using the chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Prepare the input for the model\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate the response\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "    # Extract generated IDs and decode the response\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(\"\\n ------------------------------- end translation ----------------------------------------------\")\n",
    "\n",
    "    # Display the translation\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, WebSocket, File, Form, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Add CORS Middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Allow all origins (replace '*' with specific domains in production)\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  # Allow all HTTP methods\n",
    "    allow_headers=[\"*\"],  # Allow all headers\n",
    ")\n",
    "\n",
    "active_connections = {}\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Welcome to the AI Assistant API!\"}\n",
    "\n",
    "@app.websocketwebsocket(\"/ws/{process_id}\")\n",
    "async def websocket_endpoint(websocket: WebSocket, process_id: str):\n",
    "    if not process_id or process_id == \"undefined\":  \n",
    "        await websocket.close(code=403)  # Close the connection gracefully\n",
    "        return\n",
    "    print(f\"New connection with process_id: {process_id}\")\n",
    "    await websocket.accept()\n",
    "    active_connections[process_id] = websocket\n",
    "    try:\n",
    "        while True:\n",
    "            await websocket.receive_text()\n",
    "    except Exception as e:\n",
    "        print(f\"WebSocket error: {e}\")\n",
    "    finally:\n",
    "        # Clean up the active connections\n",
    "        active_connections.pop(process_id, None)\n",
    "        await websocket.close()\n",
    "\n",
    "@app.post(\"/upload-pdf\")\n",
    "async def upload_user_pdf(file: UploadFile = File(...), store_type: str = Form(...),  indexName: str = Form(...), process_id: str = Form(...)):\n",
    "    if process_id in active_connections:\n",
    "        websocket = active_connections[process_id]\n",
    "        response = await upload_pdf(file, store_type, indexName, websocket)\n",
    "        return {\"message\": response}\n",
    "    else:\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid process_id or WebSocket connection not active\")\n",
    "\n",
    "\n",
    "\n",
    "class QueryInput(BaseModel):\n",
    "    query: str\n",
    "    index_name: str\n",
    "    model_name: str\n",
    "    search_type: str\n",
    "\n",
    "@app.post(\"/query/\")\n",
    "async def query_index(input: QueryInput):\n",
    "    query = input.query\n",
    "    collection_name = input.index_name\n",
    "    model_name = input.model_name\n",
    "    search_type = input.search_type\n",
    "    response, unique_resources, StackTrace = get_generate_response(query, collection_name, model_name, search_type)\n",
    "    return {\"response\": response, \"sources\": unique_resources, \"stackTrace\": StackTrace}\n",
    "\n",
    "class TranslateQuery(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.post(\"/translate/\")\n",
    "async def translate_query(input: TranslateQuery):\n",
    "    query = input.query\n",
    "    translated_query = get_translation(query)\n",
    "    return {\"translatedQuery\": translated_query}\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6435798,
     "sourceId": 10388219,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
